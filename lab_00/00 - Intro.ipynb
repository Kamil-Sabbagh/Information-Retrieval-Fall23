{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95edd0c6-b44e-40d6-9e9c-48f42ddd5207",
   "metadata": {},
   "source": [
    "# 0. [OPTIONAL] Installing course dependencies\n",
    "\n",
    "These are dependencies for the whole course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1d575-f278-450f-aa6f-d99e075a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158874a2-fb2f-4429-8c13-01457e9767b4",
   "metadata": {},
   "source": [
    "You may skip the next block for now. You will need `ffmpeg` on week 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f337db2-7f6a-4520-b51e-f38f064ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda update -y base conda\n",
    "!conda install -c conda-forge ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2f0dd-1d63-412c-ae93-2f012c3d8f0c",
   "metadata": {},
   "source": [
    "Run the next cell if you want to download embedding model, but this is not required during this lab. You can do it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa01d2-dd4d-48d4-a27b-9ae610d9e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_trf_distilbertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7bedc-d559-437e-b106-12c860acfbab",
   "metadata": {},
   "source": [
    "# 1. Touching the Internet\n",
    "\n",
    "Solve the following task.\n",
    "1. Download [this page](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt)\n",
    "2. Save it to the file with the **unique** name derived from the URL. NB File with another URL should not be save into the file with this name. E.g. [this file](https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt) is another file with another content!\n",
    "\n",
    "Hints:\n",
    "- [requests](https://docs.python-requests.org/en/latest/) library is cool.\n",
    "- [hashlib](https://docs.python.org/3/library/hashlib.html) may help with computing hash strings.\n",
    "- when you download and save the data, don't try to encode and decode it. Use binary format when working with streams and files. <span style=\"color:red\">Discuss with your TA which encodings you know and how they differ</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639fb2d7-d577-4ae6-beb4-2487213024cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<md5 _hashlib.HASH object @ 0x7fe5147a5d30>\n",
      "e2d9cd700ca8f8c01cd68193a1249d29030a1782108055b444a5ead7ab310425\n",
      "<Response [200]>\n",
      "<md5 _hashlib.HASH object @ 0x7fe51db63ad0>\n",
      "7770f104796093e196fac1e6822438ba9a2dadc33030edb373caf84a4fc99005\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "\n",
    "url1 = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "url2 = \"https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt\"\n",
    "\n",
    "# TODO: download and save these documents\n",
    "def save_webpage(url):\n",
    "    respone = requests.get(url)\n",
    "\n",
    "    print(respone)\n",
    "    # Ensure that the response is valid\n",
    "    respone.raise_for_status()\n",
    "\n",
    "    #Generate a uniqe hash from the URL\n",
    "    #print(hashlib.md5(url.encode()))\n",
    "    url_hash =hashlib.sha256(url.encode()).hexdigest()\n",
    "    print(url_hash)\n",
    "\n",
    "    #Save the content to the file names ny the hash:\n",
    "    with open( url_hash + \".txt\" , \"wb\") as file:\n",
    "        file.write(respone.content)\n",
    "\n",
    "\n",
    "save_webpage(url1)\n",
    "save_webpage(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7601aaf-3534-42cd-bb33-665d2d92c79d",
   "metadata": {},
   "source": [
    "# 2. Parsing different formats\n",
    "\n",
    "Most probably, if you meet something in the Internet, this is one of: binary, plain text, XML, or json. XML then splits into xHTML, RSS, Atom, SOAP, XML-RPC, ... . Your task is to learn, how to process different formats.\n",
    "\n",
    "## 2.1. JSON\n",
    "\n",
    "In [the given file](http://sprotasov.ru/data/postnauka.txt) there is valid json. Parse this file and print all video URLs, which have `computer science` tag. Use built-in features of `requests`, or just a `json` library ([ref](https://docs.python.org/3/library/json.html)).\n",
    "\n",
    "Hint:\n",
    "- if the file has issues with parsing read about [the difference](https://stackoverflow.com/questions/57152985/what-is-the-difference-between-utf-8-and-utf-8-sig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628c2c18-b896-40f9-bac9-2fefc5027da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.tech_tutorialhub.com/lessons/video_1\n",
      "http://www.coding_learnnow.com/topics/video_2\n",
      "http://www.coding_learnnow.com/courses/video_3\n",
      "http://www.coding_videostore.com/courses/video_4\n",
      "http://www.cs_tutorialhub.com/topics/video_5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL of the JSON file\n",
    "URL = \"https://raw.githubusercontent.com/YusufRoshdy/information-retrieval/main/datasets/unique_videos.json\"\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Check if the response status code is 200 (OK)\n",
    "if response.status_code == 200:\n",
    "    # Load the JSON data from the response content\n",
    "    data = json.loads(response.content)\n",
    "    \n",
    "    # Iterate through each video in the data\n",
    "    for video in data['videos']:\n",
    "        # Check if \"computer science\" is one of the tags\n",
    "        if 'computer science' in video['tags']:\n",
    "            # Print the URL\n",
    "            print(video['url'])\n",
    "else:\n",
    "    print(\"Failed to fetch the JSON file!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97530a0-46d4-47e3-a7bb-ca479680007d",
   "metadata": {},
   "source": [
    "## 2.2. HTML\n",
    "\n",
    "For a given StackExchange answer extract logins of the contributors (who asked and who answered) with votes. [bs4](https://beautiful-soup-4.readthedocs.io/en/latest/) will help you to do the job.\n",
    "\n",
    "I can recommend to use CSS or XPath selectors. `div` elements with `post-layout` class represent answers. Inside there are `div` with `votecell` class stroring votes number and `div` with class `user-details` storing user info. My personal recommendation is to use `css selectors`, which are [documented here](https://beautiful-soup-4.readthedocs.io/en/latest/#css-selectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af5adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting BeautifulSoup4\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, BeautifulSoup4\n",
      "Successfully installed BeautifulSoup4-4.12.2 soupsieve-2.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67db473-a8f0-414c-adbc-ce0e9e4710bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodrigo de Azevedo: \n",
      "            23\n",
      "         votes\n",
      "Ittay Weiss: \n",
      "            17\n",
      "         votes\n",
      "Tomasz Bartkowiak: \n",
      "            12\n",
      "         votes\n",
      "Bart Vanderbeke: \n",
      "            4\n",
      "         votes\n",
      "Bart Vanderbeke: \n",
      "            3\n",
      "         votes\n",
      "hgfei: \n",
      "            2\n",
      "         votes\n",
      "littleO: \n",
      "            1\n",
      "         votes\n",
      "TheSHETTY-Paradise: \n",
      "            1\n",
      "         votes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://math.stackexchange.com/questions/411486/understanding-the-singular-value-decomposition-svd\")\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Finding contributors of the repository\n",
    "for post in soup.select(\".post-layout\"):\n",
    "    user = post.select_one(\".user-details a\").text\n",
    "    votes = post.select_one(\".votecell .js-vote-count\").text\n",
    "    print(f\"{user}: {votes} votes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708766b-0db3-4062-87a1-9ba96c60440b",
   "metadata": {},
   "source": [
    "# 2.3. RSS feed\n",
    "\n",
    "A lot of information is already organized in typed XML documents. Podcasts, for example, are just RSS feed. Parse [the feed of this podcast](http://sprotasov.ru/podcast/rss.xml) and print out:\n",
    "- the number of episodes\n",
    "- the length of the time span between the first and the last episodes (in days).\n",
    "\n",
    "Use [`feedparser` library for this](https://waylonwalker.com/parsing-rss-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c229a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting feedparser\n",
      "  Using cached feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "Collecting sgmllib3k\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2090f810-d706-4bb2-8c85-d485a48432a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 695\n",
      "Days between first and last episode: 8576\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from datetime import datetime\n",
    "\n",
    "rss_feed = feedparser.parse('https://waylonwalker.com/rss.xml')\n",
    "\n",
    "feeds = [entry.published_parsed  for entry in rss_feed.entries if 'published_parsed' in entry]\n",
    "start_date = min(feeds)\n",
    "end_date = max(feeds)\n",
    "days_difference = (datetime(*end_date[:6]) - datetime(*start_date[:6])).days\n",
    "print(f\"Number of episodes: {len(rss_feed.entries)}\")\n",
    "print(f\"Days between first and last episode: {days_difference}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a63b1-c106-4a13-8e55-9240e9c8418f",
   "metadata": {},
   "source": [
    "# 3. Solving simple information retrieval task\n",
    "\n",
    "According to the name, `information retrieval` is the discipline, which helps retrieves information (from unstructured sources). Thus, we will retrieve some information from [this news article](https://www.bbc.com/news/world-us-canada-59944889). Your task is to write a code, which will answer the question: **How many people die every day in the US waiting for a transplant?** Write flexible enough code. Test yourself by changing the link to [this one](https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7660c706-371b-4050-aede-e4b3e4014ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 people die every day in the US waiting for a transplant.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.bbc.com/news/world-us-canada-59944889'\n",
    "url2 = 'https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/'\n",
    "\n",
    "question = 'How many people die every day in the US waiting for a transplant?'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "text = soup.get_text()\n",
    "\n",
    "import re\n",
    "\n",
    "match = re.search(r'(\\d+) people die every day in the US waiting for a transplant', text)\n",
    "if match:\n",
    "    answer = match.group(1)\n",
    "    print(f\"{answer} people die every day in the US waiting for a transplant.\")\n",
    "else:\n",
    "    print(\"Couldn't find the information.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
