{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space with ML\n",
    "\n",
    "This lab will be devoted to the use of ML model for the needs of information retrieval and text classification.  \n",
    "\n",
    "**Searching in the curious facts database**\n",
    "\n",
    "The facts dataset is given [here](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), take a look. We want you to retrieve facts **relevant to the query** (whatever it means), for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use neural networks to embed sentences\n",
    "\n",
    "Make use of any, starting from doc2vec up to Transformers, etc. Provide all code, dependencies, installation requirements.\n",
    "\n",
    "\n",
    "- [UCE in spacy 2](https://spacy.io/universe/project/spacy-universal-sentence-encoder) (`!pip install spacy-universal-sentence-encoder`)\n",
    "- [Sentence BERT in spacy 2](https://spacy.io/universe/project/spacy-sentence-bert) (`!pip install spacy-sentence-bert`)\n",
    "- [Pretrained ðŸ¤— Transformers](https://huggingface.co/transformers/pretrained_models.html)\n",
    "- [Spacy 3 transformers](https://spacy.io/usage/embeddings-transformers#transformers-installation)\n",
    "- [doc2vec pretrained](https://github.com/jhlau/doc2vec)\n",
    "- [Some more sentence transformers](https://www.sbert.net/docs/quickstart.html)\n",
    "- [Even fasttext can do a sentence embedding](https://fasttext.cc/docs/en/python-module.html#model-object)\n",
    "\n",
    "Here should be dependency installation, download instructions and so on. With outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /Users/sprotasov/opt/anaconda3/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/sprotasov/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (58.0.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/sprotasov/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (2.6.1)\n",
      "Requirement already satisfied: numpy in /Users/sprotasov/opt/anaconda3/lib/python3.9/site-packages (from fasttext) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use the library to download (and load) the model:\n",
    "\n",
    "NB: model downloading may take time (depending on the model hosting). If you think it may take a long time, ask your TA for assistance with binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x7f91b1e7b460>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS BLOCK TAKES ~ 1h\n",
    "import fasttext, fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a function that prepares embedding of arbitrary queries\n",
    "\n",
    "Write a function, which returns a fixed-sized vector of embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    return ft.get_sentence_vector(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that embeddings are of the same size and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embed(\n",
    "            \"Some random text\"\n",
    "        ).shape == \\\n",
    "        embed(\n",
    "            \"Folks, here's a story about Minnie the Moocher. \"\n",
    "            \"She was a lowdown hoochie coocher. \"\n",
    "            \"She was the roughest, toughest frail, \"\n",
    "            \"but Minnie had a heart as big as a whale\"\n",
    "        ).shape, \"Shape should match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: here we check DISTANCE, not similarity. This similar texts should produce results close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"some text for testing\"), \n",
    "            embed(\"some text for testing\")\n",
    "        )) < 1e-4, \"Embedding should match\"\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"Cats eat mice.\"), \n",
    "            embed(\"Terminator is an autonomous cyborg, typically humanoid, originally conceived as a virtually indestructible soldier, infiltrator, and assassin.\")\n",
    "        )) > 0.2, \"Embeddings should be far\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the data\n",
    "\n",
    "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "\n",
    "#TODO read facts into a list of facts. Each fact is a separate element of array\n",
    "facts = []\n",
    "\n",
    "facts = requests.get(url).text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
      "2. McDonalds calls frequent buyers of their food \"heavy users.\"\n",
      "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
      "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
      "5. You burn more calories sleeping than you do watching television.\n"
     ]
    }
   ],
   "source": [
    "print(*facts[:5], sep='\\n')\n",
    "\n",
    "assert len(facts) == 159\n",
    "assert ('our lovely little planet') in facts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform sentences to vectors\n",
    "\n",
    "Transform the list of facts to `numpy.array` of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#TODO infer vectors\n",
    "\n",
    "sent_vecs = np.array([])\n",
    "sent_vecs = np.array([embed(f) for f in facts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent_vecs.shape[0] == len(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find closest to the query\n",
    "\n",
    "Now find 5 facts which are the closest to the query using cosine measure.\n",
    "\n",
    "### 5.1. Closest search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_closest(query, dataset, k=10):\n",
    "    # TODO your code gere\n",
    "    return np.argsort(dataset @ query)[-k:]\n",
    "    return range(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Use your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: good mood\n",
      "\n",
      "\t 84. You are 1% shorter in the evening than in the morning\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n"
     ]
    }
   ],
   "source": [
    "query = \"good mood\"\n",
    "query_vec = embed(query)\n",
    "\n",
    "print(\"Results for query:\", query)\n",
    "print()\n",
    "for k in find_k_closest(query_vec, sent_vecs, 5):\n",
    "    print(\"\\t\", facts[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure DCG@5 for the following query bucket\n",
    "```\n",
    "good mood\n",
    "gorilla\n",
    "woman\n",
    "earth\n",
    "japan\n",
    "people\n",
    "math\n",
    "```\n",
    "\n",
    "Recommend 5 facts to each of the queries. Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good mood\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 84. You are 1% shorter in the evening than in the morning\n",
      "gorilla\n",
      "\t 106. The male ostrich can roar just like a lion.\n",
      "\t 85. The elephant is the only mammal that can't jump!\n",
      "\t 107. Mountain lions can whistle.\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 139. Beetles taste like apples, wasps like pine nuts, and worms like fried bacon.\n",
      "woman\n",
      "\t 131. If a pregnant woman has organ damage, the baby in her womb sends stem cells to help repair the organ.\n",
      "\t 65. A Swedish woman lost her wedding ring, and found it 16 years later- growing on a carrot in her garden.\n",
      "\t 106. The male ostrich can roar just like a lion.\n",
      "\t 88. Earth is the only planet that is not named after a god.\n",
      "\t 146. In France, it is legal to marry a dead person.\n",
      "earth\n",
      "\t 88. Earth is the only planet that is not named after a god.\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 126. Saturn's density is low enough that the planet would float in water.\n",
      "\t 153. For every human on Earth there are 1.6 million ants.\n",
      "\t 155. On Jupiter and Saturn it rains diamonds.\n",
      "japan\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
      "\t 17. Coca-Cola would be green if coloring wasn't added to it.\n",
      "\t 66. Donald duck comics were banned from Finland because he doesn't wear pants.\n",
      "\t 77. More than 60,000 people are flying over the United States in an airplane right now.\n",
      "\t 64. In Japan, crooked teeth are considered cute and attractive.\n",
      "people\n",
      "\t 34. 95% of people text things they could never say in person.\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 109. Cows kill more people than sharks do.\n",
      "\t 10. If you believe that you're truly one in a million, there are still approximately 7,184 more people out there just like you.\n",
      "\t 87. If 33 million people held hands, they could make it all the way around the equator.\n",
      "math\n",
      "\t 97. 111,111,111 X 111,111,111 = 12,345,678,987,654,321\n",
      "\t 119. Dogs are capable of understanding up to 250 words and gestures and have demonstrated the ability to do simple mathematical calculations.\n",
      "\t 5. You burn more calories sleeping than you do watching television.\n",
      "\t 48. Chewing gum burns about 11 calories per hour.\n",
      "\t 34. 95% of people text things they could never say in person.\n"
     ]
    }
   ],
   "source": [
    "bucket = \"\"\"good mood\n",
    "gorilla\n",
    "woman\n",
    "earth\n",
    "japan\n",
    "people\n",
    "math\"\"\".split('\\n')\n",
    "\n",
    "for term in bucket:\n",
    "    print(term)\n",
    "    for k in find_k_closest(embed(term), sent_vecs, k=5)[::-1]:\n",
    "        print(\"\\t\", facts[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. [BONUS] Write your own relevance assessments and compute DCG@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@5 = 1.5029\n",
      "IDCG@5 = 2.9485\n",
      "nDCG@5 = 0.5097\n"
     ]
    }
   ],
   "source": [
    "assessments = [\n",
    "    [0, 0, 0, 1, 0], # good mood\n",
    "    [0, 0, 0, 1, 0], # gorilla\n",
    "    [1, 1, 0, 0, 0], # ...\n",
    "    [1, 1, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0]\n",
    "]\n",
    "\n",
    "optimal = [[1] * 5] * 7\n",
    "\n",
    "def dcg(rels):\n",
    "    from math import log\n",
    "    s = 0\n",
    "    for i, rel in enumerate(rels):\n",
    "        s += rel / log(1 + i + 1, 2)\n",
    "    return s\n",
    "\n",
    "dcg5 = sum([dcg(row) for row in assessments]) / len(assessments)\n",
    "idcg5 = sum([dcg(row) for row in optimal]) / len(optimal)\n",
    "\n",
    "print(f\"DCG@5 = {dcg5:.4f}\")\n",
    "print(f\"IDCG@5 = {idcg5:.4f}\")\n",
    "print(f\"nDCG@5 = {dcg5 / idcg5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
